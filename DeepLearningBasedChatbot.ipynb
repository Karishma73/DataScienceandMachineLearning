{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"BUChatbot.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YcQMi1LqJT4s"},"source":["**Introduction:**  As part of AI Project, a conversational chatbot is developed that is destined to be utilized to address international student's common queries where much of the time is spent by sending enquiries to BU Helpdesk.\n","\n","**Data set**: A Javascript Object Notation(JSON) intents file is used as a dataset. The intents file is created with a lot of possible questions that the student is probably going to ask and mapping them to possible responses.\n","\n","The tag on each intent item represents the group of each message. With this information, a neural system is prepared to take a sentence of words and characterize it as one of the tags in the intent file. By doing this a possible response from the groups is taken and responded to the user. The intents file has 105 tags which are nothing but classification classes and with nearly more than 300 questions and responses to training the Neural Network.\n","\n","The dataset is comprised of 3 different JSON attributes.\n","\n","**1.Tag**: This is unique across the dataset. These values are referred to as classes while training the model. Based on this value the questions will be classified.\n","\n","**2.Patterns**: the possible questions expected from the student and used to train the model\n","\n","**3.Responses**: The responses from the Chatbot.\n","\n","The dataset is prepared from the https://www.bournemouth.ac.uk/ website.\n","\n","**Used libraries:**\n","- numpy\n","- nltk\n","- tensorflow 2.0\n","- pandas\n","- json\n","- itertools\n","- random\n","- datetime\n","- Ipython.core\n","\n","**Dependent files**: This notebook file is dependent on dataset file\n","\n","1.BUIntents.json - Dataset\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"X7og45t__8O_"},"source":["# Code Execution Environment setup\n","This jupyter notebook file can be run in Google Colaboratory or in a Anaconda virtual environment by setting the environment variable in the below code\n","\n","**For Google Colaboratory:** set **Colab** value to the environment variable\n","\n","**For Anaconda virtual environment:** set **Local** value to the environment variable\n","\n","By default the value is set to \"Colab\". So, the Colab related code will be activated. For instance with Colab drive mounting is neccessary and nltk packages should be downloaded. These are not required in local Anaconda virtual environment.\n","To toggle between Colab and Anaconda **environment** variable is used"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"v5yjRgurBa4j","colab":{}},"source":["#set Colab for Google colaboratory or set Local for anaconda environment\n","environment = 'Colab'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bvFeUelRGZWW"},"source":["# Import libraries\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"jqSsKWUM7JTd","colab":{}},"source":["import random\n","import json\n","import itertools\n","import numpy as np\n","import pandas as pd\n","import os\n","from datetime import datetime\n","from IPython.core.display import display, HTML\n","\n","#libraries need for tensor flow\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Activation, Dropout\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.models import load_model\n","\n","#libraries need for nltk\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","wordnetlemmatizer = WordNetLemmatizer()\n","\n","#this code execution is needed only for colab\n","if(environment == \"Colab\"):\n","    #download the nltk wordnet lemmatizer to access the wordnet interface from python\n","    nltk.download('punkt')\n","    nltk.download('wordnet')\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iUpY3WqtHk9_"},"source":["#Method to load the dataset\n","This method loads the json file into dataframe."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UrL7k3gt7JTo","colab":{}},"source":["def getJSONdata(filename):    \n","    with open(filename, 'r') as file:\n","        data=file.read()\n","    return pd.DataFrame.from_dict(json.loads(data)['objectives'], orient='columns')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bVj7akd-RUGp"},"source":["# Method to convert to lower case\n","This method is used to convert the object case to lower case.We can pass list or dictionary any type to this method. The method internally usinf string.lower()"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OnxBUqmC7JTx","colab":{}},"source":["def toLower(data):\n","    return str(data).lower()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"--UwQYFDSfUo"},"source":["# Method to Match the patterns with tags\n","Each pattern is transformed into a list of words utilizing **\"nltk.word_tokenizer\"**, as opposed to having them as strings. Then the tokenized words are appended with the appropriate tag. So the model can be trained easily with the patterns belonging to classes. This method returns the matched patterns and tags."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"t1zlAgII7JT1","colab":{}},"source":["def matchPatternsWithTags(row): \n","  patterns2tags = []\n","  #looping through all the patterns\n","  for pattern in row['patterns']:      \n","    #tokenizing the patterns    \n","    tword = nltk.word_tokenize(pattern)     \n","    #matched the tag with tokenized words   \n","    patterns2tags.append((tword, row['tag']))   \n","  return patterns2tags"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Frq6xH6bVHen"},"source":["# Method to tokenize the patterns\n","Each pattern will be tokenized using **\"nltk.word_tokenize\"**. This method returns the list of tokenized words."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mXHJIA-w7JT6","colab":{}},"source":["def getTokenizedWords(row):\n","    tokenized_words = []\n","    for pattern in row['patterns']:          \n","        tword = nltk.word_tokenize(pattern)        \n","        tokenized_words.extend(tword)   \n","    return tokenized_words"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5EcPAxNUVe8p"},"source":["# Method to Lemmatization\n","Lemmatization is where the singular tokens are extracted from a sentence and attempt to lessen them to their base structure. This method apply the lemmatization on the tokenized words using **\"nltk.wordnetlemmatizer\"**  and returns the list of lemma words. This method also remove the special characters\n","such as ['!', '?', ',', '.','&']\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZXxA8uzN7JT-","colab":{}},"source":["def getLemmaWords(tokenized_words):\n","   lwords = []\n","   specialCharacters = ['!', '?', ',', '.','&']\n","   lwords.append([wordnetlemmatizer.lemmatize(toLower(w)) for w in tokenized_words if not w in specialCharacters]) \n","   result = list(itertools.chain(*lwords))   \n","   return sorted(list(set(result)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8KMFJP60gdc6"},"source":["# Mount the Google Drive\n","To access the dataset file and saved weights from Google Drive, the drive needs to be mounted. This code is executed only once in the first instance and will need to be authenticated on the Google Authentication page. Follow the instructions on the Google Authentication page to proceed further."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XLQ8jRAD7JUH","colab":{}},"source":["#This code excutes only on Colab\n","if(environment == 'Colab'):\n","  #drive mount code is for colab for first time run\n","  from google.colab import drive\n","  drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9XMoWQs1Cxce"},"source":["# Assigning the dataset and weights path\n","The below code gets the dataset and the saved weights location aumatically by looping through the root directory."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Vq3oyuvjBVvT","colab":{}},"source":["#getting the root directory path of dataset and  weights\n","modelSavedPath = ''\n","datasetpath = ''\n","for roots,dirs,files in os.walk(os.getcwd()):\n","  if('BUChatbot.ipynb' in files):\n","    modelSavedPath = roots + '/BUChatbotModel.h5'\n","    datasetpath = roots + '/BUIntents.json'\n","    break"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"T-v03aqHbdrp"},"source":["# Method to Prepare the train data and the model\n","\n","In this method, the \"train data\" is prepared using a bag of words method. All the patterns with tags are taken and compared with the pattern words from the prepared Lemma words list. If the pattern word exists in Lemma words list then it will be 1 otherwise it will 0.\n","\n","The classes list is prepared with the length of the tags. If the pattern belongs to the tag then 1 is added otherwise that existence will be 0.\n","\n","Train data is [bagofwords, classes_list]\n","\n","**X traindata = bagofwords**\n","\n","**Y traindata = classes_list**\n","\n","\n","**Preparing the Model:**\n","**Sequential Model** is used with dense and dropout layers.\n","\n","The dropout is set as 0.5. That means with 0.5 probability, it sets the input units to 0 on each update at the training time. Dropout is mainly used to avoid the overfitting of the model.\n","\n","The input layer is with 128 internal units with RELU activation function.\n","\n","The hidden layer is with 64 internal units with RELU activation function.\n","\n","The output layer internal units are equal to the number of the classes with a SoftMax activation function.\n","\n","**RELU** (The rectified linear activation unit) is used, in case of positive it will output the input directly, else it will output zero. \n","\n","**Softmax:** Softmax converts a real vector to a vector of categorical probabilities. The output vector elements range between 0 and 1, that sum up to 1. Softmax is used for the classification network last layer activation   because the result could be illuminated as a probability distribution.\n","\n","For model compilation **SGD optimizer**, **categorical_crossentropy** loss function with accuracy as metrics is used.\n","\n","The model is prepared with 50 epochs and batch size as 5 . Decay is calculated as lr/epochs\n","\n","verbose = 1, which contains a progress bar and one-line per epoch. \n","\n","The weights(BUChatbotModel.h5)are saved to the same folder as the notebook."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"qpjpY5rO7JUC","colab":{}},"source":["def prepareTrainDataAndModel(patterns2tags, lemmawords, tag):   \n","    traindata = []       \n","    #creating the empty row with length of the classes\n","    empty_row =  list([0] * len(tag)) \n","    for i in patterns2tags:\n","       bagofwords = []\n","       l_words = [wordnetlemmatizer.lemmatize(word.lower()) for word in i[0]]       \n","       #if word exists in lemma words list then add 1 else 0\n","       for word in lemmawords:\n","          bagofwords.append(1) if word in l_words else bagofwords.append(0)\n","       \n","       classes_list = list(empty_row)\n","       #change the index of the current class to 1\n","       classes_list[tag.index(i[1])] = 1   \n","       traindata.append([bagofwords, classes_list])\n","        \n","    random.shuffle(traindata)\n","    traindata = np.array(traindata)    \n","    traindata_x = list(traindata[:,0])  \n","    traindata_y = list(traindata[:,1])  \n","    \n","    #model training\n","    chatbotmodel = Sequential()\n","    chatbotmodel.add(Dense(128, input_shape=(len(traindata_x[0]),), activation='relu'))\n","    chatbotmodel.add(Dropout(0.5))   \n","    chatbotmodel.add(Dense(64, activation='relu'))\n","    chatbotmodel.add(Dropout(0.5))\n","    chatbotmodel.add(Dense(len(traindata_y[0]), activation='softmax'))\n","    \n","    sgdoptimizer = SGD(lr=0.01, decay=0.0002, momentum=0.9)\n","    chatbotmodel.compile(loss='categorical_crossentropy', optimizer=sgdoptimizer, metrics=['accuracy'])\n","    \n","    history = chatbotmodel.fit(np.array(traindata_x), np.array(traindata_y), epochs=50, batch_size=5, verbose=1)    \n","    chatbotmodel.save(modelSavedPath, history)\n","    chatbotmodel.summary()\n","   "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"p2LAy_bYmpZR"},"source":["# Method to train the model\n","\n","**The below sequence is followed to call the methods**\n","\n","1.getJSONData\n","\n","2.Call GetTokenizeWords method and add the results to a new column in data frame.\n","\n","3.Call matchPatternsWithTags method and add the results to a new column in data frame.\n","\n","4.Call the getLemmaWords method by passing the tokinized words as input parameters.\n","\n","5.Call the BagOfWordsAndModelPreparation method by passing the Patterns2Tags list and Tags list as input parameters.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vnnFUb6cGD2U","colab":{}},"source":["df = getJSONdata(datasetpath)\n","df['tokenizedwords'] = df.apply(getTokenizedWords, axis=1)\n","df['patterns2tags'] = df.apply(matchPatternsWithTags, axis=1)\n","lemmawords = getLemmaWords(list(itertools.chain(*df['tokenizedwords'])))\n","prepareTrainDataAndModel(list(itertools.chain(*df['patterns2tags'])), lemmawords, sorted(list(set(df['tag']))))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1pUQ_1Cbg6q2"},"source":["# Methods to process user input into a bag of words and predictions\n","Finally, the model is trained successfully. Now it is time to get the predictions from the saved model. As discussed earlier Neural Network only understands the numeric data format, but the user asks the question in string format.\n","\n","So the following same steps are repeated for user input.\n","\n","1. Get responses from JSON\n","2. Tokenization\n","3. Lemmatization\n","4. Bag of words\n","5. Predict the output\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nsOaZFxuiCL9"},"source":["Below code loads the saved weights from the same path of the notebook file.\n","\n","The tag names are defined as classes used for classification."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9kzdkVfF7JUL","colab":{}},"source":["classes = sorted(list(set(df['tag'])))\n","chatbotmodel = load_model(modelSavedPath)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Rn6C234jAayZ"},"source":["**Getting the Bot response for user question**\n","This method converts the user question to a bag of words and predict the class for that question.\n","The Error_threshold value is set as 0.25, which uses to filter out predictions below the threshold and provide an intent index\n","\n","The top probability prediction is picked to respond to the user."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MrjC8hZU7JUc","colab":{}},"source":["def getBotResponse(input):\n","   #tokenization of user input\n","    input_words = nltk.word_tokenize(input)   \n","    #lemmatizing the user input\n","    input_words = [wordnetlemmatizer.lemmatize(iword.lower()) for iword in input_words]  \n","    #converting input into bag of words\n","    bagofwords = [0]*len(lemmawords)  \n","    for w in input_words:\n","        for i,word in enumerate(lemmawords):\n","            if word == w:   \n","                bagofwords[i] = 1               \n","    p = np.array(bagofwords)\n","   \n","    #predicting the model\n","    result = chatbotmodel.predict(np.array([p]))[0]\n","    #threshold cutoff \n","    ERROR_THRESHOLD = 0.25\n","    #loading the predictions ins array\n","    predictions = [[i,r] for i,r in enumerate(result) if r>ERROR_THRESHOLD] \n","\n","    #sorting the predictions\n","    predictions.sort(key=lambda x: x[1], reverse=True)\n","    prediction_list = []\n","    for r in predictions:\n","        prediction_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n","\n","    #print prediction list and probability for our reference\n","    print(prediction_list)\n","    #if there is no intent bot reply below message\n","    if(len(prediction_list) == 0):\n","        return \"I am sorry, I didn't get you\"\n","\n","    tag = prediction_list[0]['intent'] \n","    responses = df.query('tag == \"'+ tag + '\"')['responses'].tolist()\n","    if(toLower(tag) == 'localtime'):     \n","       return str(eval(responses[0]))\n","\n","    if(type(responses[0]) is list):\n","      response = random.choice(responses[0])   \n","    else:\n","      response = responses[0]   \n","  \n","    return response"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pdWD34pJtzVB"},"source":["# Testing the Chatbot\n","The Following are the two ways to test the bot.\n","\n","1. Print statement\n","\n","2. IPYWidgets GUI.\n","\n","Either of the ones can be used for testing.\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pcSJC54zuBjy"},"source":["**Displaying the output using simple print statement**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HCaPkHOd7JUk","colab":{}},"source":["response = getBotResponse(\"bye\")\n","display(HTML(response))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NHR11pvZuJCV"},"source":["**Displaying the output using IPYWidgets**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sdkH4zW07JUo","colab":{}},"source":["import ipywidgets as widgets\n","from ipywidgets import GridspecLayout,Layout,VBox,HBox\n","\n","#This method will be called on send button click\n","def on_send_click(b):\n","  #get userinput\n","    usermsg = TextEntryBox.value\n","    TextEntryBox.value = ''\n","\n","    #display user message in chat display box\n","    if usermsg != '':\n","       if(ChatDisplayBox.value == ''):\n","         ChatDisplayBox.value = \"you: \" + usermsg + '</br>'         \n","       else:\n","         ChatDisplayBox.value = ChatDisplayBox.value  + \"you: \" + usermsg + '</br>'      \n","      #get the reposne\n","       response = getBotResponse(usermsg)      \n","     \n","     #display the response\n","       if(ChatDisplayBox.value == ''):\n","         ChatDisplayBox.value = \"BUHelpDesk: \" + response + '</br>' \n","       else:\n","         ChatDisplayBox.value = ChatDisplayBox.value  + \"BUHelpDesk: \" + response + '</br>'  \n","\n","# fill it in with widgets\n","layout1 = Layout(flex='0 1 auto', height='500px', min_height='500px', width='600px',overflow_y='auto')\n","layout1.border= \"2px solid lightblue\" \n","layout2 = Layout(flex='0 1 auto', height='50px', min_height='50px', width='500px')\n","layout3= Layout(flex='0 1 auto', height='30px', min_height='30px', width='100px')\n","ChatDisplayBox = widgets.HTML(value='', layout=layout1, disabled=True)\n","TextEntryBox = widgets.Text(value='',layout=layout2)\n","sendButton = widgets.Button(description='Send',layout=layout3)\n","sendButton.on_click(on_send_click)\n","vb_left = VBox([TextEntryBox], layout=Layout( width='500'))\n","vb_right = VBox([sendButton], layout=Layout( width='100'))\n","hb = HBox([vb_left,vb_right])\n","display(ChatDisplayBox)\n","hb\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qtb1sWfCwSEn","colab_type":"text"},"source":["[nltk]: http://en.wikipedia.org/wiki/Chile \"Wikipedia Article About Chile\""]}]}